# Linear Regression

1. Define function

    ![image](https://user-images.githubusercontent.com/20740522/177243503-1d79e7b2-ad38-40ca-ae5f-9ac0893554d7.png)
  
2. Define loss function

    ![image](https://user-images.githubusercontent.com/20740522/177243573-e8bc2248-46ad-498b-8fa7-93671dae7e91.png)
    
    ![image](https://user-images.githubusercontent.com/20740522/177683833-8476cec0-8660-4d09-87d0-fa8ad8b104a0.png)
  
3. Perform optimization 
    - Batch gradient descent
      - Differentiate L(θ) wrt θj (to get the gradient corresponding to each θj)

          ![image](https://user-images.githubusercontent.com/20740522/177243895-e1fba245-68ef-412b-80f6-89350d2bf236.png)

      - Update each θj in the negative gradient direction to minimize the loss function

          ![image](https://user-images.githubusercontent.com/20740522/177243962-b1e00e59-eeaf-457f-a9a2-9e56251aba1e.png)
    - Stochastic gradient descent
      - Differentiate L<sup>*</sup>(θ) wrt θj (to get the gradient corresponding to each θj)
      - Update each θj in the negative gradient direction to minimize the loss function

          ![image](https://user-images.githubusercontent.com/20740522/177683924-9548cf69-aee7-4d1c-9278-ea9b295712c7.png)
          
    Note: Learning rate can be fixed (high hinders convergence, lower slows convergence) or variable (depending on where we are and type of features)

